{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheet 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from numpy import linalg as LA\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Principal Component Analysis\n",
    "### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement PCA (fill in the blanks in the function below)\n",
    "\n",
    "\n",
    "def pca(data, n_components=None):\n",
    "    \"\"\"\n",
    "    Principal Component Analysis on a p x N data matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.ndarray\n",
    "        Data matrix of shape (p, N).\n",
    "    n_components : int, optional\n",
    "        Number of requested components. By default returns all components.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray, np.ndarray\n",
    "        the pca components (shape (n_components, p)) and the projection (shape (n_components, N))\n",
    "\n",
    "    \"\"\"\n",
    "    # set n_components to p by default\n",
    "    n_components = data.shape[0] if n_components is None else n_components\n",
    "    assert (\n",
    "        n_components <= data.shape[0]\n",
    "    ), f\"Got n_components larger than dimensionality of data!\"\n",
    "\n",
    "    # center the data\n",
    "    mean = np.mean(data, axis=1, keepdims=True)\n",
    "    X = data - mean\n",
    "\n",
    "    # compute X times X transpose\n",
    "    covmat = X @ X.T\n",
    "\n",
    "    # compute the eigenvectors and eigenvalues\n",
    "    eigval, eigvec = LA.eig(covmat)\n",
    "    assert all(eigval > 0)\n",
    "    assert np.isreal(eigvec).all()\n",
    "\n",
    "    # sort the eigenvectors by eigenvalue and take the n_components largest ones\n",
    "    indices = np.argsort(eigval)\n",
    "    # weirdly, numpy docs don't tell us if the it's decreasing or increasing:\n",
    "    if eigval[indices[0]] < eigval[indices[-1]]:\n",
    "        indices = indices[::-1]\n",
    "    components = eigvec[:, indices[:n_components]].T\n",
    "    eigval = eigval[indices]\n",
    "\n",
    "    # compute X_projected, the projection of the data to the components\n",
    "    X_projected = components @ X\n",
    "\n",
    "    return (\n",
    "        components,\n",
    "        X_projected,\n",
    "    )  # return the n_components first components and the pca projection of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data to test your implementation\n",
    "# All the asserts on the bottom should go through if your implementation is correct\n",
    "\n",
    "data = np.array(\n",
    "    [[1, 0, 0, -1, 0, 0], [0, 3, 0, 0, -3, 0], [0, 0, 5, 0, 0, -5]], dtype=np.float32\n",
    ")\n",
    "\n",
    "# add a random offset to all samples. it should not affect the results\n",
    "data += np.random.randn(data.shape[0], 1)\n",
    "\n",
    "n_components = 2\n",
    "components, projection = pca(\n",
    "    data, n_components=n_components\n",
    ")  # apply your implementation\n",
    "\n",
    "# the correct results are known (up to some signs)\n",
    "true_components = np.array([[0, 0, 1], [0, 1, 0]], dtype=np.float32)\n",
    "true_projection = np.array([[0, 0, 5, 0, 0, -5], [0, 3, 0, 0, -3, 0]], dtype=np.float32)\n",
    "\n",
    "# check that components match, up to sign\n",
    "assert isinstance(\n",
    "    components, np.ndarray\n",
    "), f\"Expected components to be numpy array but got {type(components)}\"\n",
    "assert (\n",
    "    components.shape == true_components.shape\n",
    "), f\"{components.shape}!={true_components.shape}\"\n",
    "assert np.allclose(\n",
    "    np.abs(components * true_components).sum(1), np.ones(n_components)\n",
    "), f\"Components not matching\"\n",
    "\n",
    "# check that projections agree, taking into account potentially flipped components\n",
    "assert isinstance(\n",
    "    projection, np.ndarray\n",
    "), f\"Expected projection to be numpy array but got {type(projection)}\"\n",
    "assert projection.shape == (\n",
    "    n_components,\n",
    "    data.shape[1],\n",
    "), f\"Incorrect shape of projection: Expected {(n_components, data.shape[1])}, got {projection.shape}\"\n",
    "assert np.allclose(\n",
    "    projection,\n",
    "    true_projection * (components * true_components).sum(1, keepdims=True),\n",
    "    atol=1e-6,\n",
    "), f\"Projections not matching\"\n",
    "\n",
    "print(\"Test successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data (it is a subset of the data at https://opendata.cern.ch/record/4910#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.shape=(116, 2233), labels.shape=(2233,)\n",
      "3 unique labels: [0. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "features = np.load(\"data/dijet_features.npy\")\n",
    "labels = np.load(\"data/dijet_labels.npy\")\n",
    "label_names = [\"b\", \"c\", \"q\"]  # bottom, charm or light quarks\n",
    "\n",
    "print(f\"{features.shape=}, {labels.shape=}\")  # print the shapes\n",
    "\n",
    "# print how many samples of each class are present in the data (hint: numpy.unique)\n",
    "unique_labels = np.unique(labels)\n",
    "print(f\"{len(unique_labels)} unique labels: {unique_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min(feat_ranges)=0.021708244174450164; max(feat_ranges)=2576772.606923904\n",
      "(116,)\n"
     ]
    }
   ],
   "source": [
    "# report range of features and normalize the data to zero mean and unit variance\n",
    "feat_ranges = features.max(axis=1) - features.min(axis=1)\n",
    "print(f'{min(feat_ranges)=}; {max(feat_ranges)=}')\n",
    "normalized = features - np.mean(features, axis=1, keepdims=True)\n",
    "normalized = normalized / np.std(features, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "Compute a 2D PCA projection and make a scatterplot of the result, once without color, once coloring the dots by label. Interpret your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# TODO: apply PCA as implemented in (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# TODO: make a scatterplot of the PCA projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# TODO: make a scatterplot, coloring the dots by their label and including a legend with the label names\n",
    "# (hint: one way is to call plt.scatter once for each of the three possible labels. Why could it be problematic to scatter the data sorted by labels though?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Nonlinear Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap  # import umap-learn, see https://umap-learn.readthedocs.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have not done 1(b) yet, you can load the normalized features directly:\n",
    "features = np.load(\"data/dijet_features_normalized.npy\")\n",
    "labels = np.load(\"data/dijet_labels.npy\")\n",
    "label_names = [\"b\", \"c\", \"q\"]  # bottom, charm or light quarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply umap on the normalized jet features from excercise 1. It will take a couple of seconds.\n",
    "# note: umap uses a different convention regarding the feature- and sample dimension, N x p instead of p x N!\n",
    "\n",
    "reducer = umap.UMAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make a scatterplot of the UMAP projection\n",
    "\n",
    "# TODO: make a scatterplot, coloring the dots by their label and including a legend with the label names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_neighbors in (2, 4, 8, 15, 30, 60, 100):\n",
    "    # TODO: repeat the above, varying the n_neighbors parameter of UMAP\n",
    "    reducer = umap.UMAP(n_neighbors=n_neighbors)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
